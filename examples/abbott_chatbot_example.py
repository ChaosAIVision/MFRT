"""
Abbott Milk Sales Chatbot - Dataset & Evaluation Flow

This example demonstrates:
1. Creating a dataset for Abbott milk sales chatbot
2. Using evaluator to automatically generate feedback
3. Optimizing the chatbot prompt based on feedback
"""

import asyncio
import pandas as pd
from chaos_auto_prompt.optimizers import PromptLearningOptimizer
from chaos_auto_prompt.evaluators import ClassificationEvaluator
from chaos_auto_prompt.config import get_settings

settings = get_settings()


# Dataset: 10 customer questions about Abbott milk products
# Real scenarios from Vietnamese market
dataset = pd.DataFrame([
    {
        "customer_question": "S·ªØa Abbott n√†o t·ªët cho tr·∫ª 1 tu·ªïi?",
        "chatbot_answer": "Abbott Grow c√≥ th·ªÉ ph√π h·ª£p cho b√© 1 tu·ªïi."
    },
    {
        "customer_question": "Gi√° s·ªØa Ensure Gold bao nhi√™u?",
        "chatbot_answer": "Kho·∫£ng 800,000 VND/h·ªôp 850g."
    },
    {
        "customer_question": "S·ªØa Similac c√≥ t·ªët cho tr·∫ª s∆° sinh kh√¥ng?",
        "chatbot_answer": "C√≥, Similac ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát cho tr·∫ª s∆° sinh v·ªõi h·ªá ti√™u h√≥a non y·∫øu."
    },
    {
        "customer_question": "T√¥i mu·ªën mua s·ªØa cho b√† ngo·∫°i 70 tu·ªïi, n√™n ch·ªçn lo·∫°i n√†o?",
        "chatbot_answer": "Ensure Gold l√† l·ª±a ch·ªçn t·ªët cho ng∆∞·ªùi cao tu·ªïi."
    },
    {
        "customer_question": "S·ªØa PediaSure c√≥ gi√∫p tr·∫ª tƒÉng c√¢n kh√¥ng?",
        "chatbot_answer": "PediaSure ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát ƒë·ªÉ h·ªó tr·ª£ tƒÉng tr∆∞·ªüng chi·ªÅu cao v√† c√¢n n·∫∑ng cho tr·∫ª bi·∫øng ƒÉn."
    },
    {
        "customer_question": "C√≥ giao h√†ng t·∫≠n nh√† kh√¥ng?",
        "chatbot_answer": "V√¢ng."
    },
    {
        "customer_question": "S·ªØa Abbott Grow v√† Similac kh√°c nhau th·∫ø n√†o?",
        "chatbot_answer": "Similac cho tr·∫ª 0-3 tu·ªïi, Grow cho tr·∫ª l·ªõn h∆°n."
    },
    {
        "customer_question": "T√¥i c√≥ th·ªÉ ƒë·ªïi tr·∫£ n·∫øu con kh√¥ng u·ªëng ƒë∆∞·ª£c kh√¥ng?",
        "chatbot_answer": "C√≥ th·ªÉ ƒë·ªïi tr·∫£ trong v√≤ng 7 ng√†y n·∫øu s·∫£n ph·∫©m ch∆∞a m·ªü n·∫Øp v√† c√≤n nguy√™n tem."
    },
    {
        "customer_question": "S·ªØa Ensure c√≥ b·ªã ti·ªÉu ƒë∆∞·ªùng u·ªëng ƒë∆∞·ª£c kh√¥ng?",
        "chatbot_answer": "Ensure c√≥ d√≤ng Ensure Diabetes Care d√†nh ri√™ng cho ng∆∞·ªùi ti·ªÉu ƒë∆∞·ªùng v·ªõi ƒë∆∞·ªùng huy·∫øt ·ªïn ƒë·ªãnh."
    },
    {
        "customer_question": "Khuy·∫øn m√£i g√¨ trong th√°ng n√†y?",
        "chatbot_answer": "Mua 2 t·∫∑ng 1 cho t·∫•t c·∫£ d√≤ng s·ªØa Abbott."
    }
])

print("üìä Abbott Milk Chatbot Dataset")
print("=" * 80)
print(f"Total samples: {len(dataset)}")
print("\nSample questions:")
for i, row in dataset.head(3).iterrows():
    print(f"\n{i+1}. Q: {row['customer_question']}")
    print(f"   A: {row['chatbot_answer']}")
print("\n" + "=" * 80)


async def main():
    print("\nü§ñ STEP 1: Creating Evaluator for Abbott Chatbot")
    print("=" * 80)

    # Create evaluator to assess chatbot quality
    evaluator = ClassificationEvaluator(
        feedback_column="quality",
        model=settings.openai_default_model,  # Use model from .env
        prompt_template="""
        B·∫°n l√† chuy√™n gia ƒë√°nh gi√° chatbot b√°n h√†ng.

        C√¢u h·ªèi kh√°ch h√†ng: {customer_question}
        C√¢u tr·∫£ l·ªùi chatbot: {chatbot_answer}

        ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng c√¢u tr·∫£ l·ªùi d·ª±a tr√™n:
        1. ƒê·ªô ch√≠nh x√°c th√¥ng tin v·ªÅ s·∫£n ph·∫©m Abbott
        2. T√≠nh chuy√™n nghi·ªáp v√† th√¢n thi·ªán
        3. ƒê·ªô chi ti·∫øt ph√π h·ª£p
        4. Kh·∫£ nƒÉng t∆∞ v·∫•n b√°n h√†ng

        Tr·∫£ v·ªÅ JSON:
        {{
            "quality": "excellent" ho·∫∑c "good" ho·∫∑c "poor",
            "explanation": "gi·∫£i th√≠ch ng·∫Øn g·ªçn t·∫°i sao ƒë√°nh gi√° nh∆∞ v·∫≠y v√† g·ª£i √Ω c·∫£i thi·ªán"
        }}
        """,
        choices={"excellent": 2, "good": 1, "poor": 0},
        include_explanation=True
    )

    print("‚úÖ Evaluator created")
    print(f"   - Model: {settings.openai_default_model}")
    print(f"   - Feedback columns: quality, explanation")
    print(f"   - Concurrency: 20 (parallel evaluation)")

    # Initial prompt (not very good)
    initial_prompt = """B·∫°n l√† chatbot b√°n s·ªØa Abbott. Tr·∫£ l·ªùi: {customer_question}"""

    print(f"\nüìù Initial Prompt: '{initial_prompt}'")

    # Create optimizer
    optimizer = PromptLearningOptimizer(
        prompt=initial_prompt,
        model_choice=settings.openai_default_model,  # Use model from .env
        budget_limit=2.0,
        verbose=True
    )

    print("\n" + "=" * 80)
    print("üîç STEP 2: Running Evaluator (Generating Feedback)")
    print("=" * 80)

    # Run evaluator to generate feedback
    dataset_with_feedback, feedback_cols = await optimizer.run_evaluators(
        dataset=dataset,
        evaluators=[evaluator]
    )

    print("\n‚úÖ Feedback generated!")
    print(f"   Feedback columns: {feedback_cols}")

    # Show feedback results
    print("\nüìä Evaluation Results:")
    print("-" * 80)
    for i, row in dataset_with_feedback.head(5).iterrows():
        print(f"\n{i+1}. Q: {row['customer_question'][:50]}...")
        print(f"   Quality: {row['quality']}")
        print(f"   Explanation: {row['explanation'][:80]}...")

    # Quality distribution
    quality_counts = dataset_with_feedback['quality'].value_counts()
    print("\nüìà Quality Distribution:")
    for quality, count in quality_counts.items():
        print(f"   {quality}: {count} samples")

    print("\n" + "=" * 80)
    print("üöÄ STEP 3: Optimizing Prompt")
    print("=" * 80)

    # Optimize prompt
    optimized_prompt = await optimizer.optimize(
        dataset=dataset_with_feedback,
        output_column="chatbot_answer",
        feedback_columns=feedback_cols
    )

    print("\n" + "=" * 80)
    print("üìä RESULTS")
    print("=" * 80)

    print(f"\nüî¥ BEFORE (Initial Prompt):")
    print(f"   {initial_prompt}")

    print(f"\nüü¢ AFTER (Optimized Prompt):")
    print(f"   {optimized_prompt}")

    print(f"\nüí∞ Cost Summary:")
    print(f"   Total cost: ${optimizer.pricing_calculator.get_total_cost():.4f}")
    print(f"   Budget used: {optimizer.pricing_calculator.get_total_cost() / optimizer.pricing_calculator.budget_limit * 100:.1f}%")
    print(f"   Remaining: ${optimizer.pricing_calculator.get_remaining_budget():.4f}")

    usage = optimizer.pricing_calculator.get_usage_summary()
    print(f"\nüìà Token Usage:")
    print(f"   Input tokens: {usage['total_input_tokens']:,}")
    print(f"   Output tokens: {usage['total_output_tokens']:,}")
    print(f"   Total tokens: {usage['total_tokens']:,}")

    print("\n" + "=" * 80)
    print("‚úÖ OPTIMIZATION COMPLETE!")
    print("=" * 80)

    # Test with new prompt
    print("\nüß™ STEP 4: Testing Optimized Prompt")
    print("=" * 80)

    test_question = "S·ªØa n√†o t·ªët nh·∫•t cho b√© 2 tu·ªïi bi·∫øng ƒÉn?"
    print(f"\n‚ùì Test Question: {test_question}")
    print(f"\nNow you can use the optimized prompt to generate better responses!")

    return {
        "initial_prompt": initial_prompt,
        "optimized_prompt": optimized_prompt,
        "dataset_with_feedback": dataset_with_feedback,
        "cost": optimizer.pricing_calculator.get_total_cost(),
        "usage": usage
    }


if __name__ == "__main__":
    print("\n" + "üè• ABBOTT MILK CHATBOT - PROMPT OPTIMIZATION ".center(80, "="))
    print("Using Evaluator Pattern with LLM-as-a-Judge\n")

    results = asyncio.run(main())

    print(f"\nüíæ Results saved to memory. You can access:")
    print(f"   - results['initial_prompt']")
    print(f"   - results['optimized_prompt']")
    print(f"   - results['dataset_with_feedback']")
